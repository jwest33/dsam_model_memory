# Copy this file to .env and adjust the values as needed
#
# Note: All settings have defaults in ConfigManager. Only override what you need.
# Settings can also be configured via the web UI at http://localhost:5001/config

# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================

# Web Interface Port (Flask)
AM_WEB_PORT=5001

# API Wrapper Port (FastAPI)
AM_API_PORT=8001

# Llama.cpp Server Port
AM_LLAMA_PORT=8000

# Cache settings
AM_CACHE_TTL=300  # Cache time-to-live in seconds (5 minutes)

# Rate limiting (optional)
AM_RATE_LIMIT_REQUESTS=100  # Max requests per window
AM_RATE_LIMIT_WINDOW=60     # Time window in seconds

# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================

# Path to GGUF model file (required)
# Windows example: C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-Q8_0.gguf
# Linux/Mac example: /home/user/models/Qwen3-4B-Instruct-2507-Q8_0.gguf
AM_MODEL_PATH=
# Alternative: LLM_MODEL_PATH= (legacy, AM_MODEL_PATH takes precedence)

# Model alias/name
AM_LLM_MODEL=Qwen3-4b-instruct-2507

# LLM API base URL (usually auto-configured)
AM_LLM_BASE_URL=http://localhost:8000/v1

# ==============================================================================
# LLAMA.CPP SERVER CONFIGURATION
# ==============================================================================

# Option 1: Set the direct path to llama-server executable
# LLAMA_SERVER_PATH=C:\Users\YourName\llama.cpp\build\bin\Release\llama-server.exe

# Option 2: Set the path to your llama.cpp directory
# LLAMA_CPP_PATH=C:\Users\YourName\llama.cpp

# Llama.cpp server parameters (optional)
# LLM_CONTEXT_SIZE=20480  # Context size for llama.cpp server (overrides AM_CONTEXT_WINDOW)
# LLM_GPU_LAYERS=999      # Number of layers to offload to GPU (999 = all)
# LLM_THREADS=8           # Number of CPU threads to use
# LLM_MODEL=model-alias   # Model alias for llama.cpp server

# ==============================================================================
# LLM GENERATION DEFAULTS
# ==============================================================================

# Default temperature for generation (0.0-2.0, lower = more focused)
AM_DEFAULT_TEMPERATURE=0.3

# Default max tokens for generation
AM_DEFAULT_MAX_TOKENS=100

# Default repetition penalty (1.0 = no penalty)
AM_DEFAULT_REPETITION_PENALTY=1.2

# Advanced sampling parameters
AM_DEFAULT_TOP_P=0.9
AM_DEFAULT_TOP_K=40

# ==============================================================================
# MEMORY SYSTEM CONFIGURATION
# ==============================================================================

# Database path
AM_DB_PATH=./amemory.sqlite3

# FAISS index path
AM_INDEX_PATH=./faiss.index

# Embedding model (Hugging Face model name)
# Options:
#   - sentence-transformers/all-MiniLM-L6-v2 (default, 384 dims)
#   - sentence-transformers/all-mpnet-base-v2 (768 dims, better quality)
#   - sentence-transformers/multi-qa-MiniLM-L6-cos-v1 (384 dims, QA-optimized)
AM_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Context window size
AM_CONTEXT_WINDOW=8192

# Reserved tokens
AM_RESERVE_OUTPUT_TOKENS=1024  # Reserved for LLM output
AM_RESERVE_SYSTEM_TOKENS=512   # Reserved for system prompts

# ==============================================================================
# RETRIEVAL CONFIGURATION
# ==============================================================================

# Enable attention-based dynamic retrieval (recommended)
AM_USE_ATTENTION=true

# Enable liquid memory clustering (recommended)
AM_USE_LIQUID_CLUSTERS=true

# Enable multi-part extraction for complex content
AM_USE_MULTI_PART=true

# Multi-part extraction threshold (minimum characters)
AM_MULTI_PART_THRESHOLD=200

# MMR Lambda for diversity in retrieval (0.0-1.0)
AM_MMR_LAMBDA=0.5

# ==============================================================================
# RETRIEVAL WEIGHTS (used when AM_USE_ATTENTION=false)
# Note: These weights must sum to 1.0
# ==============================================================================

AM_W_SEMANTIC=0.68   # Semantic similarity weight
AM_W_LEXICAL=0.20    # Lexical/keyword match weight
AM_W_RECENCY=0.10    # Recency bias weight
AM_W_ACTOR=0.07      # Actor relevance weight
AM_W_SPATIAL=0.03    # Spatial proximity weight
AM_W_USAGE=0.05      # Usage pattern weight

# ==============================================================================
# MEMORY MANAGEMENT
# ==============================================================================

# Maximum memories before pruning
AM_MEMORY_BUDGET=20480

# ==============================================================================
# CLUSTERING PARAMETERS (Advanced)
# ==============================================================================

# Cluster flow rate (how quickly memories flow between clusters)
AM_CLUSTER_FLOW_RATE=0.1

# Cluster merge threshold (similarity required to merge)
AM_CLUSTER_MERGE_THRESHOLD=0.85

# Cluster energy decay rate
AM_CLUSTER_ENERGY_DECAY=0.99

# ==============================================================================
# LEARNING PARAMETERS (Advanced)
# ==============================================================================

# Hebbian learning rate for memory connections
AM_HEBBIAN_RATE=0.01

# Synaptic decay rate for unused connections
AM_SYNAPTIC_DECAY=0.001

# Embedding momentum rate
AM_EMBED_MOMENTUM=0.95

# Drift blend ratio (how much drift affects embeddings)
AM_DRIFT_BLEND=0.3

# ==============================================================================
# FILE UPLOAD CONFIGURATION
# ==============================================================================

# Maximum upload file size in bytes (50MB default)
AM_MAX_UPLOAD_SIZE=52428800

# Allowed file extensions (comma-separated)
AM_UPLOAD_EXTENSIONS=txt,pdf,docx,md,html,json,csv,xml,log,py,js,java,cpp,c,h

# ==============================================================================
# ADVANCED SETTINGS
# ==============================================================================

# Embedding dimension (depends on embedding model)
AM_EMBED_DIM=384

# Number of attention heads
AM_ATTENTION_HEADS=8

# Consolidation state path (for memory consolidation tracking)
AM_CONSOLIDATION_PATH=./consolidation_state.pkl

# ==============================================================================
# REMOTE SERVER CONFIGURATION (Optional)
# ==============================================================================

# If using a remote llama.cpp server instead of local
# LLM_SERVER_HOST=remote-server.example.com
# LLM_SERVER_PORT=8000

# ==============================================================================
# DEVELOPMENT/DEBUG SETTINGS
# ==============================================================================

# Test mode (set to 1 for testing)
# AM_TEST_MODE=0

# Startup timeout for llama server (seconds)
# LLM_STARTUP_TIMEOUT=60
