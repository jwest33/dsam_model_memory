# LLM Server Configuration
# ========================

# Model Configuration
LLM_MODEL_PATH=C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-F16.gguf
LLM_MODEL=qwen3-4b-instruct-2507-f16
LLM_CONTEXT_SIZE=10000
LLM_GPU_LAYERS=999  # Use ALL layers on GPU for maximum speed
LLM_THREADS=8  # Increase thread count

# Memory System Configuration
AM_LLM_BASE_URL=http://localhost:8000/v1
AM_LLM_MODEL=Qwen3-4b-instruct-2507
AM_CONTEXT_WINDOW=8192
AM_DB_PATH=./data/amemory.sqlite3
AM_INDEX_PATH=./data/faiss.index

# Retrieval Weights (must sum to 1.0)
AM_W_SEMANTIC=0.68
AM_W_RECENCY=0.02
AM_W_ACTOR=0.10
AM_W_SPATIAL=0.10
AM_W_USAGE=0.05
