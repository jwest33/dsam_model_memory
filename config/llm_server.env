# LLM Server Configuration
# This file contains environment variables for configuring the llama.cpp server

# Model Settings
LLM_MODEL_PATH=C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-F16.gguf
LLM_MODEL=qwen3-4b-instruct-2507-f16

# Server Settings
LLM_HOST=0.0.0.0
LLM_PORT=8000

# Context and Processing
# Higher context allows longer conversations but uses more VRAM
# Context size (tokens) - adjust based on your VRAM
# 4GB VRAM: 2048-4096, 8GB: 8192, 12GB+: 10000+
LLM_CONTEXT_SIZE=10000
LLM_THREADS=12

# GPU/VRAM Optimization
# Number of layers to offload to GPU (-1 for all, 0 for CPU-only)
LLM_GPU_LAYERS=35

# Keep model locked in memory (prevents swapping to disk)
# Recommended: true for dedicated GPU servers
LLM_LOCK_MEMORY=true

# Disable memory mapping (loads entire model into RAM/VRAM)
# true = faster inference but uses more memory upfront
# false = uses memory mapping (default, more memory efficient)
LLM_NO_MMAP=false

# Offload K,Q,V cache to GPU (uses more VRAM but faster)
# Note: In newer llama.cpp versions, this is controlled by --n-gpu-layers
# LLM_OFFLOAD_KQV=true

# Use Flash Attention (if your GPU supports it)
# Reduces memory usage and increases speed for long contexts
LLM_FLASH_ATTENTION=false

# Batch processing sizes (affects speed vs memory tradeoff)
# Higher values = faster but more memory usage
LLM_BATCH_SIZE=512
LLM_UBATCH_SIZE=512

# Multi-GPU Settings (if applicable)
# LLM_MAIN_GPU=0
# LLM_GPU_SPLIT=0.5,0.5
# LLM_TENSOR_SPLIT=0.5,0.5

# Timeouts
LLM_STARTUP_TIMEOUT=90

# Remote Server (optional)
# Uncomment to use a remote llama.cpp server instead of local
# LLM_SERVER_HOST=192.168.1.100
# LLM_SERVER_PORT=8000
