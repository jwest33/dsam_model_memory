Use a **dual-space encoder** with **field-aware composition** and **lightweight, on-the-fly adaptation**:

* **Euclidean space** ‚Üí local semantics & lexical nuance (great for retrieval).
* **Hyperbolic space** ‚Üí latent hierarchy & symbolism (great for motifs/archetypes).
* **Field-aware heads** for 5W1H so ‚Äúwhat vs. why vs. how‚Äù contribute differently.
* **Adaptive residuals** that learn from co-access (your ‚Äúgravitational‚Äù update), but **don‚Äôt move the base embedding**‚Äîadd a *bounded residual* instead.
* Query-time clustering + graph centrality stays, but runs on **product distance** that mixes Euclidean + Hyperbolic terms.

Think: **Product manifold = ùîº^d √ó ‚Ñç^h**, composed per 5W1H, with small residuals that evolve from usage.

---

# 1) Encoder: field-aware, dual-space

**Backbone**: any strong sentence encoder (you‚Äôre already using `all-MiniLM-L6-v2`; for richer semantics consider `e5-large`/`gte-large`).
**Heads**:

* **Euclidean head**: linear ‚Üí L2-normalized vector `u ‚àà ùîº^d`.
* **Hyperbolic head**: linear ‚Üí tangent‚Üíexp map (Poincar√© ball) ‚Üí `v ‚àà ‚Ñç^h`.

**Field tokens & gates**
For each 5W1H field, prepend a learned \[WHO], \[WHAT], ‚Ä¶ token to its text, run the same backbone, then project:

```
u_field_i = U(Enc([FIELD_i] + text_i))   # Euclidean
v_field_i = Exp‚àòH(Enc([FIELD_i] + text_i)) # Hyperbolic (Exp map)
```

Then **compose with learned gates** (initialized from your config weights):

```
Œ±_i = softplus(w_i)                       # learnable importance per field
u_base = L2( Œ£_i Œ±_i ¬∑ u_field_i )
v_base = ‚äï_i ( Œ±_i ‚äô v_field_i )         # gyro-sum / M√∂bius addition
```

> Result: each memory gets `(u_base, v_base)`.

Why this matters:

* Field tokens make ‚Äúwhy‚Äù naturally separate from ‚Äúwhat‚Äù.
* Hyperbolic head **pulls out hierarchies & symbolism** that span documents (archetypes, motifs).
* You can keep your current model and add these heads as a thin MLP.

---

# 2) Storage format (don‚Äôt mutate the anchor)

For each memory `m` store:

```
u_anchor, v_anchor        # immutable base (from encoder)
Œîu, Œîv                    # small adaptive residuals (start at 0)
meta: 5W1H text, timestamps, episode id, etc.
```

At retrieval time:

```
u_eff = L2(u_anchor + Œîu)
v_eff = ‚äï(v_anchor, Œîv)   # hyperbolic residual via M√∂bius addition
```

> **Never** overwrite `*_anchor`. Track your drift metric exactly as you do, but now it‚Äôs drift of the *residual*, not the base.

---

# 3) Distance & ranking (product metric)

Use a **product distance** with query-conditioned weights:

```
D(q, m) = Œª_E ¬∑ (1 - cos(u_q, u_eff_m))  +  Œª_H ¬∑ d_H(v_q, v_eff_m)
```

* `Œª_E, Œª_H` are **query-dependent** (e.g., if query has clear ‚Äúwhat/where‚Äù, upweight Euclidean; if it‚Äôs thematic/why/how, upweight Hyperbolic).
* Feed these Œª‚Äôs from a tiny MLP over query field presence/length.

You can still do:

1. ANN prefilter in **Euclidean** (fast IVF+PQ),
2. Re-rank top-K with **product metric**,
3. Build the similarity graph ‚Üí DBSCAN/HDBSCAN ‚Üí eigenvector centrality.

---

# 4) ‚ÄúGravitational‚Äù adaptation done right

Your current idea (momentum SGD pulling co-accessed items together) is great. Just apply it to **residuals**, not anchors, and keep it **bounded**:

**Euclidean residual (with EMA momentum & clip):**

```python
force_u = relevance * (u_partner - u_self)
v_u = Œº * v_u + Œ∑ * force_u
Œîu = clip_by_norm(Œîu + v_u, max_norm=Œ¥_E)
```

**Hyperbolic residual (on the manifold):**

* Compute tangent update at `v_eff` via **log map**, step, then **exp back**; clip geodesic norm by `Œ¥_H`.

**Safety rails**

* Per-item caps `||Œîu|| ‚â§ Œ¥_E`, `d_H(Œîv, 0) ‚â§ Œ¥_H`.
* **Anchored L2** penalty in training discourages large residuals unless reinforced.
* **Aging**: residuals decay (EMA to zero) if not re-affirmed.

This yields your ‚Äúco-access = gravity‚Äù without catastrophic drift.

---

# 5) Training signals (no labels required)

Even if you never fine-tune the backbone, you can train **just the heads & gates** with self-supervision from your system logs:

**Positives (q, m‚Å∫):**

* Click/selection & ‚Äúpositive feedback‚Äù.
* Co-access in the same recall (within a short window).
* Episode adjacency / temporal proximity.
* Same entity/WHO co-mentions across events.

**Negatives (q, m‚Åª):**

* High-scoring but rejected items.
* Items from distant episodes with disjoint WHO/WHEN.

**Loss (multi-objective):**

* **InfoNCE** on Euclidean head.
* **Geodesic margin** on hyperbolic head (bring motifs together).
* **Uniformity** regularizer (keep space spread).
* **Field gate regularizer**: small L2 to stop a single field from dominating.
* **Drift regularizer**: small L2 on residual norms.

You can train offline nightly on logs; online you only update residuals.

---

# 6) Symbolism surfacing (the fun part)

To make motifs pop out:

* Build a **metaphor/analogy probe** for queries: extract antonym/analogy axes (e.g., light‚Üîdark, order‚Üîchaos) with a small set of seed pairs; project candidates onto these axes and add a **symbolic affinity bonus** to ranking.
* In the hyperbolic space, **hierarchical clustering** across the entire corpus (offline) produces **motif hubs**; store hub IDs and add a **hub-cohesion score** at re-rank time.
* Track **edge types** in the candidate graph (co-access, same WHO, same WHY keyword family) and weight centrality by edge semantics to emphasize symbolic links over mere lexical overlap.

---

# 7) Query-time clustering (slightly upgraded)

* Switch to **HDBSCAN** if density varies; or keep DBSCAN but set `eps` via the 10-NN elbow of the candidate set.
* Build the graph on **product similarity**; threshold separately per cluster using **Otsu** on edge weights to avoid a global threshold.
* Rank by **centrality √ó query-affinity** (product distance inverse) √ó **symbolic hub bonus**.

---

# 8) Scale & ops tips

* **ChromaDB**: enable **IVF+PQ** (or OPQ) for Euclidean prefilter; store hyperbolic vectors in a side collection (or just store tangent vectors‚Äîcompute geodesic distance on re-rank).
* Keep **two indexes**: fast Euclidean ANN for top-K; small RAM re-ranker for product metric & clustering.
* Log **(Œª\_E, Œª\_H, residual norms, acceptance rates)** to auto-tune gates and drift caps.

---

# 9) Minimal implementation sketch

**Encode & compose**

```python
# Encode each field once
tokens = {f: f"[{f.upper()}] " + text for f, text in five_w1h.items() if text}

u_fields = {f: U(Enc(tokens[f])) for f in tokens}         # Euclidean head + L2
v_fields = {f: Exp(H(Enc(tokens[f]))) for f in tokens}    # Hyperbolic head

Œ± = softplus(w)  # dict of field weights, learnable
u_anchor = L2(sum(Œ±[f] * u_fields[f] for f in u_fields))
v_anchor = mobius_sum([(Œ±[f], v_fields[f]) for f in v_fields])  # Poincar√© sum
store(u_anchor, v_anchor, Œîu=0, Œîv=0)
```

**Query distance & re-rank**

```python
Œª_E, Œª_H = gate_from_query(q_meta)  # tiny MLP on which 5W1H present
cands = ann_search_Euclidean(u_q, K=512)
for m in cands:
    u_eff, v_eff = u_anchor[m]+Œîu[m], mobius_add(v_anchor[m], Œîv[m])
    d = Œª_E*(1 - cosine(u_q, u_eff)) + Œª_H*geo_dist(v_q, v_eff)
    score[m] = -d + symbolic_bonus(q, m)  # e.g., hub proximity, axis alignment
top = rerank_by_graph_clustering(score)
```

**Adaptive residual update (positive association between a and b)**

```python
def adapt_pair(a, b, relevance):
    # Euclidean
    fu = relevance * (u_eff[b] - u_eff[a])
    v_u[a] = Œº*v_u[a] + Œ∑*fu
    Œîu[a] = clip_norm(Œîu[a] + v_u[a], Œ¥_E)

    # Hyperbolic (log-exp around a)
    ta = log_map(v_eff[a], v_eff[b])         # tangent at a toward b
    tv[a] = Œº*tv[a] + Œ∑*relevance*ta
    Œîv[a] = clip_geo( mobius_add(Œîv[a], exp_map0(tv[a])), Œ¥_H )
```

---

# 10) What this buys you (relative to ‚Äúplain embeddings‚Äù)

* **Abstract symbolism** emerges in **‚Ñç** even when Euclidean similarities are weak.
* **Field-aware** lets you weight ‚Äúwhy/how‚Äù more for thematic queries.
* **Adaptation is safe**: you never corrupt anchors; residuals are bounded, decayed, and explainable.
* **Unlimited storage** stays viable: Euclidean ANN scales, product metric only touches top-K.

---

## Sensible starting hyperparams

* `d=768` Euclidean, `h=64` Hyperbolic.
* Residual caps: `Œ¥_E = 0.35` cosine-norm, `Œ¥_H = 0.75` geodesic.
* Momentum `Œº=0.9`, step `Œ∑ ‚àà [1e-3, 1e-2]` gated by relevance.
* Initial field weights (from your config): who=1.0, what=2.0, when=0.5, where=0.5, why=1.5, how=1.0 (learnable).
