# LLM Server Configuration
# ========================

# Option 1: Set the direct path to llama-server executable
# LLAMA_SERVER_PATH=C:\Users\YourName\llama.cpp\build\bin\Release\llama-server.exe

# Option 2: Set the path to your llama.cpp directory
# LLAMA_CPP_PATH=C:\Users\YourName\llama.cpp

# Model Configuration
LLM_MODEL_PATH=C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-F16.gguf
LLM_MODEL=qwen3-4b-instruct-2507-f16
LLM_CONTEXT_SIZE=10000
LLM_GPU_LAYERS=99
LLM_THREADS=4

# Memory System Configuration
AM_LLM_BASE_URL=http://localhost:8000/v1
AM_LLM_MODEL=Qwen3-4b-instruct-2507
AM_CONTEXT_WINDOW=8192
AM_DB_PATH=./amemory.sqlite3
AM_INDEX_PATH=./faiss.index

# Retrieval Weights (must sum to 1.0)
AM_W_SEMANTIC=0.55
AM_W_LEXICAL=0.20
AM_W_RECENCY=0.10
AM_W_ACTOR=0.07
AM_W_SPATIAL=0.03
AM_W_USAGE=0.05

# Optional: Web Search API Keys
# SERPER_API_KEY=your_key_here
# SEARCH_API_KEY=your_key_here
